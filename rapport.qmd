---
title: "Rapport de laboratoire 3"
subtitle: "MTH8408"
author:
  - name: Votre nom
    email: votre.adresse@polymtl.ca
    affiliation:
      - name: Polytechnique Montréal
format:
  pdf:
    keep-tex: false
    documentclass: article
    include-in-header:
      - text: |
            \usepackage{eulervm}
            \usepackage{xspace}
            \usepackage[francais]{babel}
    geometry:
      - margin=1in
    papersize: letter
    colorlinks: true
    urlcolor: blue
engine: julia
---

```{julia}
#| output: false
using Pkg
Pkg.activate("labo3_env")
using LinearAlgebra
```

# Contexte

Dans ce laboratoire, on demande d'implémenter deux méthodes basées sur la méthode de Newton pour le problème
$$
  \min_x \ f(x)
$$ {#eq-nlp}
où $f : \mathbb{R}^n \to \mathbb{R}$ est deux fois continûment différentiable.

# Question 1

En cours, nous avons vu la méthode de Newton modifiée avec recherche linéaire inexacte pour résoudre ([-@eq-nlp]).

Dans cette question, on demande d'implémenter et de tester cette méthode *en utilisant une factorisation modifiée* de $\nabla^2 f(x_k)$.

Votre implémentation doit avoir les caractéristiques suivantes :

1. prendre un `AbstractNLPModel` en argument ;
1. un critère d'arrêt absolu et relatif sur le gradient de l'objectif ;
2. un critère d'arrêt portant sur le nombre d'itérations (le nombre maximum d'itérations devrait dépendre du nombre de variables $n$ du problème) ;
2. toujours démarrer de l'approximation initiale spécifiée par le modèle ;
3. allouer un minimum en utilisant les opérations vectorisées (`.=`, `.+`, `.+=`, etc.) autant que possible ;
6. votre fonction principale doit être documentée---reportez-vous à [https://docs.julialang.org/en/v1/manual/documentation](https://docs.julialang.org/en/v1/manual/documentation) ;
7. votre fonction doit faire afficher les informations pertinentes à chaque itération sous forme de tableau comme vu en cours.

Tester votre implémentation sur le problème polynomial vu en classe et les problèmes non linéaires de la section *Problèmes test* ci-dessous.

```{julia}
function newton_modifiee(model, eps_a=1.0e-5, eps_r=1.0e-5)
  # votre code ici
end
```

# Question 2

Dans cette question, on demande d'implémenter la méthode de Newton inexacte pour résoudre ([-@eq-nlp]).

Votre implémentation doit avoir les mêmes caractéristiques qu'à la question 1.

Il faut de plus ajuster votre méthode de manière à encourager la convergence locale superlinéaire.

Tester votre implémentation sur le problème polynomial vu en classe et les problèmes non linéaires de la section *Problèmes test* ci-dessous.

```{julia}
function newton_inexacte(model, eps_a=1.0e-5, eps_r=1.0e-5)
  # votre code ici
end
```

# Résultats numériques

## Problèmes test

Votre premier problème test sera le problème polynomial vu en classe.

```{julia}
# f(x) = ...
# model = ADNLPModel(...)
```

Utiliser ensuite les problèmes non linéaires du dépôt `OptimizationProblems.jl` qui sont sans contraintes et ont 100 variables.
Vous pouvez y accéder à l'aide de l'extrait de code suivant :
```{julia}
#| output: false
Pkg.add("OptimizationProblems")  # collection + outils pour sélectionner les problèmes
using OptimizationProblems, OptimizationProblems.ADNLPProblems

meta = OptimizationProblems.meta
problem_list = meta[(meta.ncon.==0).&.!meta.has_bounds.&(meta.nvar.==100), :name]
problems = (OptimizationProblems.ADNLPProblems.eval(Meta.parse(problem))() for problem ∈ problem_list)
```

Parmis ces problèmes, choisissez-en 3 et illustrez le comportement de chacune des méthodes.

## Validation de la méthode de Newton modifiée

```{julia}
# votre code ici
```

## Validation de la méthode de Newton tronquée

```{julia}
# votre code ici
```

## Commentaires sur les résultats

<!-- Insérer ici votre évaluation des résultats -->
